{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "FOLDER_INPUT_PATH = \"../data_students/labeled_data/\"\n",
    "FOLDER_OUTPUT_PATH = \"../data_students/labeled_data/img_features/\"\n",
    "\n",
    "FOLDER_INPUT_PATH_PREDICT = \"../data_students/unlabeled_data/\"\n",
    "FOLDER_OUTPUT_PATH_PREDICT = \"../data_students/unlabeled_data/\"\n",
    "\n",
    "def get_variable_name(variable, namespace):\n",
    "    return [name for name, value in namespace.items() if value is variable][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping borders for train images: 100%|██████████| 1000/1000 [00:00<00:00, 3764.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped images saved to ../data_students/labeled_data/Img_train_cropped/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping borders for test images: 100%|██████████| 500/500 [00:00<00:00, 4933.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped images saved to ../data_students/labeled_data/Img_test_cropped/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping borders for predict images: 100%|██████████| 500/500 [00:00<00:00, 5069.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped images saved to ../data_students/unlabeled_data/Img_cropped/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def crop_images(mode, border_size=5):\n",
    "    if mode not in ['train', 'test', 'predict']:\n",
    "        raise ValueError(\"Mode must be either 'train' or 'test'.\")\n",
    "    \n",
    "    if mode == 'predict':\n",
    "        input_folder_predict = f\"{FOLDER_INPUT_PATH_PREDICT}Img/\"\n",
    "        output_folder_predict = f\"{FOLDER_OUTPUT_PATH_PREDICT}Img_cropped/\"\n",
    "\n",
    "        if not os.path.exists(output_folder_predict):\n",
    "            os.makedirs(output_folder_predict)\n",
    "\n",
    "        image_files_predict = [f for f in sorted(os.listdir(input_folder_predict)) if f.endswith('.png')]\n",
    "        for filename in tqdm(image_files_predict, desc=f\"Cropping borders for {mode} images\"):\n",
    "            img_path = os.path.join(input_folder_predict, filename)\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            width, height = img.size\n",
    "\n",
    "            cropped_img = img.crop((\n",
    "                border_size,  # Left\n",
    "                border_size,  # Top\n",
    "                width - border_size,  # Right\n",
    "                height - border_size  # Bottom\n",
    "            ))\n",
    "            cropped_img.save(os.path.join(output_folder_predict, filename))\n",
    "\n",
    "        print(f\"Cropped images saved to {output_folder_predict}\")\n",
    "\n",
    "    else :\n",
    "        input_folder = f\"{FOLDER_INPUT_PATH}Img_{mode}/\"\n",
    "        output_folder = f\"{FOLDER_INPUT_PATH}Img_{mode}_cropped/\"\n",
    "\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        image_files = [f for f in sorted(os.listdir(input_folder)) if f.endswith('.png')]\n",
    "        for filename in tqdm(image_files, desc=f\"Cropping borders for {mode} images\"):\n",
    "            img_path = os.path.join(input_folder, filename)\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            width, height = img.size\n",
    "\n",
    "            cropped_img = img.crop((\n",
    "                border_size,  # Left\n",
    "                border_size,  # Top\n",
    "                width - border_size,  # Right\n",
    "                height - border_size  # Bottom\n",
    "            ))\n",
    "            cropped_img.save(os.path.join(output_folder, filename))\n",
    "\n",
    "        print(f\"Cropped images saved to {output_folder}\")\n",
    "\n",
    "crop_images('train')\n",
    "crop_images('test')\n",
    "crop_images('predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening cropped images for train: 100%|██████████| 1000/1000 [00:00<00:00, 17159.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened images for train saved to ../data_students/labeled_data/img_features/img_feat_flat_train.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening cropped images for test: 100%|██████████| 500/500 [00:00<00:00, 21769.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened images for test saved to ../data_students/labeled_data/img_features/img_feat_flat_test.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening cropped images for predict: 100%|██████████| 500/500 [00:00<00:00, 17274.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened images for predict saved to ../data_students/unlabeled_data/img_features/img_feat_flat_predict.csv.\n"
     ]
    }
   ],
   "source": [
    "def flatten_images(mode, cropped=True):\n",
    "    if mode not in ['train', 'test', 'predict']:\n",
    "        raise ValueError(\"Mode must be either 'train' or 'test'.\")\n",
    "\n",
    "    if mode == 'predict':\n",
    "        if cropped:\n",
    "            input_folder = f\"{FOLDER_INPUT_PATH_PREDICT}Img_cropped/\"\n",
    "        else:\n",
    "            input_folder = f\"{FOLDER_INPUT_PATH_PREDICT}Img/\"\n",
    "        output_csv = f\"{FOLDER_OUTPUT_PATH_PREDICT}img_features/img_feat_flat_{mode}.csv\"\n",
    "\n",
    "        flattened_images = []\n",
    "        filenames = []\n",
    "\n",
    "        image_files = [f for f in os.listdir(input_folder) if f.endswith('.png')]\n",
    "        image_files.sort()\n",
    "\n",
    "        for filename in tqdm(image_files, desc=f\"Flattening {'cropped ' if cropped else ''}images for {mode}\"):\n",
    "            img_path = os.path.join(input_folder, filename)\n",
    "            img = Image.open(img_path)\n",
    "            img_gray = img.convert('L')\n",
    "            img_array = np.array(img_gray).flatten()\n",
    "\n",
    "            flattened_images.append(img_array)\n",
    "            filenames.append(filename)\n",
    "\n",
    "        if len(flattened_images) > 0:\n",
    "            num_pixels = len(flattened_images[0])\n",
    "            column_names = [f'pixel_{i + 1}' for i in range(num_pixels)]\n",
    "        else:\n",
    "            raise ValueError(\"No valid images found in the folder.\")\n",
    "        \n",
    "        flattened_images_df = pd.DataFrame(flattened_images, columns=column_names)\n",
    "        flattened_images_df['img_filename'] = filenames\n",
    "\n",
    "        flattened_images_df.to_csv(output_csv, index=False)\n",
    "        print(f\"Flattened images for {mode} saved to {output_csv}.\")\n",
    "\n",
    "    else :\n",
    "        if cropped:\n",
    "            input_folder = f\"{FOLDER_INPUT_PATH}Img_{mode}_cropped/\"\n",
    "        else:\n",
    "            input_folder = f\"{FOLDER_INPUT_PATH}Img_{mode}/\"\n",
    "        output_csv = f\"{FOLDER_OUTPUT_PATH}img_feat_flat_{mode}.csv\"\n",
    "\n",
    "        flattened_images = []\n",
    "        filenames = []\n",
    "\n",
    "        image_files = [f for f in os.listdir(input_folder) if f.endswith('.png')]\n",
    "        image_files.sort()\n",
    "\n",
    "        for filename in tqdm(image_files, desc=f\"Flattening {'cropped ' if cropped else ''}images for {mode}\"):\n",
    "            img_path = os.path.join(input_folder, filename)\n",
    "            img = Image.open(img_path)\n",
    "            img_gray = img.convert('L')\n",
    "            img_array = np.array(img_gray).flatten()\n",
    "\n",
    "            flattened_images.append(img_array)\n",
    "            filenames.append(filename)\n",
    "\n",
    "        if len(flattened_images) > 0:\n",
    "            num_pixels = len(flattened_images[0])\n",
    "            column_names = [f'pixel_{i + 1}' for i in range(num_pixels)]\n",
    "        else:\n",
    "            raise ValueError(\"No valid images found in the folder.\")\n",
    "\n",
    "        flattened_images_df = pd.DataFrame(flattened_images, columns=column_names)\n",
    "        flattened_images_df['img_filename'] = filenames\n",
    "\n",
    "        flattened_images_df.to_csv(output_csv, index=False)\n",
    "        print(f\"Flattened images for {mode} saved to {output_csv}.\")\n",
    "\n",
    "flatten_images('train')\n",
    "flatten_images('test')\n",
    "flatten_images('predict')\n",
    "# flatten_images('train', cropped=False)\n",
    "# flatten_images('test', cropped=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import hog, corner_harris, corner_peaks, canny\n",
    "from skimage.io import imread\n",
    "\n",
    "\n",
    "def extract_image_features_hog(image_path):\n",
    "    image = imread(image_path, as_gray=True)\n",
    "    features, _ = hog(image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_image_features_orb(image_path):\n",
    "    resize_dim=(224, 224)\n",
    "    n_keypoints=500\n",
    "    max_features = 500\n",
    "\n",
    "    image = imread(image_path)\n",
    "\n",
    "    if len(image.shape) == 2:  # Grayscale images\n",
    "        gray_image = image\n",
    "    else:\n",
    "        gray_image = rgb2gray(image)\n",
    "        gray_image = (gray_image * 255).astype(np.uint8)\n",
    "\n",
    "    resized_image = cv2.resize(gray_image, resize_dim, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    orb = cv2.ORB_create(n_keypoints)\n",
    "    _, descriptors = orb.detectAndCompute(resized_image, None)\n",
    "\n",
    "    if descriptors is None:\n",
    "        return np.zeros((n_keypoints * 32,), dtype=np.float32)  # Default ORB descriptor size is 32\n",
    "\n",
    "    descriptors = descriptors.flatten()\n",
    "\n",
    "    if descriptors.shape[0] < max_features:\n",
    "        padded_descriptors = np.zeros((max_features,), dtype=np.float32)\n",
    "        padded_descriptors[:descriptors.shape[0]] = descriptors\n",
    "        return padded_descriptors\n",
    "    else:\n",
    "        return descriptors[:max_features]\n",
    "\n",
    "def extract_image_features_harris(image_path):\n",
    "    image = imread(image_path)\n",
    "\n",
    "    if len(image.shape) == 2:\n",
    "        gray_image = image\n",
    "    else:\n",
    "        gray_image = rgb2gray(image)\n",
    "\n",
    "    corners = corner_peaks(corner_harris(gray_image), min_distance=5)\n",
    "\n",
    "    num_corners = len(corners)\n",
    "    features = np.array([num_corners], dtype=np.float32)\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_image_features_canny(image_path):\n",
    "    image = imread(image_path)\n",
    "\n",
    "    if len(image.shape) == 2:\n",
    "        gray_image = image\n",
    "    else:\n",
    "        gray_image = rgb2gray(image)\n",
    "\n",
    "    edges = canny(gray_image, sigma=1.0)\n",
    "\n",
    "    edge_percentage = np.sum(edges) / edges.size\n",
    "    features = np.array([edge_percentage], dtype=np.float32)\n",
    "    return features\n",
    "\n",
    "def extract_image_features_sift(image_path):\n",
    "    resize_dim=(224, 224)\n",
    "    n_features=500\n",
    "    descriptor_size=200\n",
    "    \n",
    "    image = imread(image_path)\n",
    "    if len(image.shape) == 2:  # Grayscale images\n",
    "        gray_image = image\n",
    "    else:\n",
    "        gray_image = rgb2gray(image)\n",
    "        gray_image = (gray_image * 255).astype(np.uint8)\n",
    "\n",
    "    resized_image = cv2.resize(gray_image, resize_dim, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    sift = cv2.SIFT_create(nfeatures=n_features)\n",
    "    _, descriptors = sift.detectAndCompute(resized_image, None)\n",
    "\n",
    "    if descriptors is None:\n",
    "        return np.zeros((descriptor_size,), dtype=np.float32)\n",
    "\n",
    "    descriptors = descriptors.flatten()\n",
    "    if descriptors.size < descriptor_size:\n",
    "        padded_descriptors = np.zeros((descriptor_size,), dtype=np.float32)\n",
    "        padded_descriptors[:descriptors.size] = descriptors\n",
    "        return padded_descriptors\n",
    "    else:\n",
    "        return descriptors[:descriptor_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train images: 100%|██████████| 1000/1000 [00:00<00:00, 3702.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features for train saved to ../data_students/labeled_data/img_features/img_feat_ced_train.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test images: 100%|██████████| 500/500 [00:00<00:00, 4017.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features for test saved to ../data_students/labeled_data/img_features/img_feat_ced_test.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing predict images: 100%|██████████| 500/500 [00:00<00:00, 2415.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features for predict saved to ../data_students/unlabeled_data/img_features/img_feat_ced_predict.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_features(mode, extractor, cropped=True):\n",
    "    if mode not in ['train', 'test', 'predict']:\n",
    "        raise ValueError(\"Mode must be either 'train' or 'test'.\")\n",
    "    \n",
    "    if mode == 'predict':\n",
    "        if cropped:\n",
    "            image_folder = f\"{FOLDER_INPUT_PATH_PREDICT}Img_cropped/\"\n",
    "        else:\n",
    "            image_folder = f\"{FOLDER_INPUT_PATH_PREDICT}Img/\"\n",
    "        output_feature_csv = f\"{FOLDER_OUTPUT_PATH_PREDICT}img_features/img_feat_{extractor}_{mode}.csv\"\n",
    "\n",
    "        feature_list = []\n",
    "        filenames = []\n",
    "\n",
    "        image_files = [img_file for img_file in sorted(os.listdir(image_folder)) if img_file.endswith('.png')]\n",
    "\n",
    "        for img_file in tqdm(image_files, desc=f\"Processing {mode} images\"):\n",
    "            img_path = os.path.join(image_folder, img_file)\n",
    "            \n",
    "            features = None\n",
    "            if extractor == 'hog':\n",
    "                features = extract_image_features_hog(img_path)\n",
    "            elif extractor == 'orb':\n",
    "                features = extract_image_features_orb(img_path)\n",
    "            elif extractor == 'harris':\n",
    "                features = extract_image_features_harris(img_path)\n",
    "            elif extractor == 'ced':\n",
    "                features = extract_image_features_canny(img_path)\n",
    "            feature_list.append(features)\n",
    "            filenames.append(img_file)\n",
    "\n",
    "    for img_file in tqdm(image_files, desc=f\"Processing {mode} images\"):\n",
    "        img_path = os.path.join(image_folder, img_file)\n",
    "        \n",
    "        features = None\n",
    "        if extractor == 'hog':\n",
    "            features = extract_image_features_hog(img_path)\n",
    "        elif extractor == 'orb':\n",
    "            features = extract_image_features_orb(img_path)\n",
    "        elif extractor == 'harris':\n",
    "            features = extract_image_features_harris(img_path)\n",
    "        elif extractor == 'ced':\n",
    "            features = extract_image_features_canny(img_path)\n",
    "        elif extractor == 'sift':\n",
    "            features = extract_image_features_sift(img_path)\n",
    "        feature_list.append(features)\n",
    "        filenames.append(img_file)\n",
    "\n",
    "    else : \n",
    "        if cropped:\n",
    "            image_folder = f\"{FOLDER_INPUT_PATH}Img_{mode}_cropped/\"\n",
    "        else:\n",
    "            image_folder = f\"{FOLDER_INPUT_PATH}Img_{mode}/\"\n",
    "        output_feature_csv = f\"{FOLDER_OUTPUT_PATH}img_feat_{extractor}_{mode}.csv\"\n",
    "\n",
    "        feature_list = []\n",
    "        filenames = []\n",
    "\n",
    "        image_files = [img_file for img_file in sorted(os.listdir(image_folder)) if img_file.endswith('.png')]\n",
    "\n",
    "        for img_file in tqdm(image_files, desc=f\"Processing {mode} images\"):\n",
    "            img_path = os.path.join(image_folder, img_file)\n",
    "            \n",
    "            features = None\n",
    "            if extractor == 'hog':\n",
    "                features = extract_image_features_hog(img_path)\n",
    "            elif extractor == 'orb':\n",
    "                features = extract_image_features_orb(img_path)\n",
    "            elif extractor == 'harris':\n",
    "                features = extract_image_features_harris(img_path)\n",
    "            elif extractor == 'ced':\n",
    "                features = extract_image_features_canny(img_path)\n",
    "            feature_list.append(features)\n",
    "            filenames.append(img_file)\n",
    "\n",
    "        feature_df = pd.DataFrame(feature_list)\n",
    "        feature_df['img_filename'] = filenames\n",
    "        feature_df.to_csv(output_feature_csv, index=False)\n",
    "        print(f\"Image features for {mode} saved to {output_feature_csv}.\")\n",
    "\n",
    "\n",
    "# create_features('train', 'hog')\n",
    "# create_features('test', 'hog')\n",
    "create_features('train', 'orb')\n",
    "create_features('test', 'orb')\n",
    "# create_features('train', 'harris')\n",
    "# create_features('test', 'harris')\n",
    "# create_features('train', 'ced')\n",
    "# create_features('test', 'ced')\n",
    "# create_features('train', 'sift')\n",
    "# create_features('test', 'sift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, label_file, transform=None):\n",
    "        self.image_paths = sorted(list(Path(image_folder).glob(\"*.png\")))\n",
    "        self.labels = pd.read_csv(label_file, header=None).values.flatten()\n",
    "        self.transform = transform\n",
    "        assert len(self.image_paths) == len(self.labels), \"Mismatch between images and labels!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor([label], dtype=torch.float32)\n",
    "    \n",
    "class ImagesDatasetUnlabeled(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_paths = sorted(list(Path(image_folder).glob(\"*.png\")))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "        if hasattr(dataset, 'image_paths'):\n",
    "            self.image_paths = [dataset.image_paths[i] for i in indices]\n",
    "        else:\n",
    "            self.image_paths = [None] * len(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = self.indices[idx]\n",
    "        image, label = self.dataset[original_idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),          # Randomly flip the image horizontally\n",
    "    transforms.RandomVerticalFlip(p=0.5),            # Randomly flip the image vertically\n",
    "    transforms.RandomRotation(degrees=15),           # Randomly rotate the image by up to 15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2,           # Randomly adjust brightness, contrast, and saturation\n",
    "                           contrast=0.2,\n",
    "                           saturation=0.2,\n",
    "                           hue=0.1),\n",
    "    transforms.Grayscale(num_output_channels=1),    # Convert to grayscale\n",
    "    transforms.Resize((32, 32)),                    # Resize to match the model's input size\n",
    "    transforms.ToTensor(),                          # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))            # Normalize to [-1, 1]\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))            # Normalize to [-1, 1]\n",
    "])\n",
    "train_pretrained_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_folder = f\"{FOLDER_INPUT_PATH}Img_train_cropped/\"\n",
    "train_label_file = f\"{FOLDER_INPUT_PATH}y_train.csv\"\n",
    "test_image_folder = f\"{FOLDER_INPUT_PATH}Img_test_cropped/\"\n",
    "test_label_file = f\"{FOLDER_INPUT_PATH}y_test.csv\"\n",
    "\n",
    "predict_image_folder = f\"{FOLDER_INPUT_PATH_PREDICT}Img_cropped/\"\n",
    "\n",
    "\n",
    "train_dataset = ImagesDataset(train_image_folder, train_label_file, transform=transform)\n",
    "test_dataset = ImagesDataset(test_image_folder, test_label_file, transform=transform)\n",
    "predict_dataset = ImagesDatasetUnlabeled(predict_image_folder, transform=transform)\n",
    "\n",
    "train_split_ratio = 0.8\n",
    "train_size = int(train_split_ratio * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_indices, val_indices = torch.utils.data.random_split(\n",
    "    range(len(train_dataset)), [train_size, val_size]\n",
    ")\n",
    "\n",
    "train_subset = TransformDataset(train_dataset, train_indices, transform=None)\n",
    "val_subset = TransformDataset(train_dataset, val_indices, transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_folder = f\"{FOLDER_INPUT_PATH}Img_train_cropped/\"\n",
    "train_label_file = f\"{FOLDER_INPUT_PATH}y_train.csv\"\n",
    "test_image_folder = f\"{FOLDER_INPUT_PATH}Img_test_cropped/\"\n",
    "test_label_file = f\"{FOLDER_INPUT_PATH}y_test.csv\"\n",
    "\n",
    "\n",
    "train_dataset = ImagesDataset(train_image_folder, train_label_file, transform=None)\n",
    "test_dataset = ImagesDataset(test_image_folder, test_label_file, transform=val_transform)\n",
    "\n",
    "train_split_ratio = 0.8\n",
    "train_size = int(train_split_ratio * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_indices, val_indices = torch.utils.data.random_split(\n",
    "    range(len(train_dataset)), [train_size, val_size]\n",
    ")\n",
    "\n",
    "train_subset = TransformDataset(train_dataset, train_indices.indices, transform=train_transform)\n",
    "val_subset = TransformDataset(train_dataset, train_indices.indices, transform=val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, learning_rate, num_epochs, pretrained=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Training]\")\n",
    "        \n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix(train_loss=(train_loss / (progress_bar.n + 1)))\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                if pretrained:\n",
    "                    outputs = torch.sigmoid(model(images))\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch + 1} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        checkpoint_path = f\"pretrained/{type(model).__name__}_e{epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Model weights saved for epoch {epoch + 1} as '{checkpoint_path}'\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(train_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def test_model(model, test_loader, pretrained=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            if pretrained:\n",
    "                outputs = torch.sigmoid(model(images))\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total * 100 if total > 0 else 0\n",
    "    print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "    return test_loss / len(test_loader), accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResNet18_Weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[1;32m      3\u001b[0m NUM_CLASSES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 5\u001b[0m resnet18 \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mresnet18(weights\u001b[38;5;241m=\u001b[39m\u001b[43mResNet18_Weights\u001b[49m\u001b[38;5;241m.\u001b[39mIMAGENET1K_V1)\n\u001b[1;32m      6\u001b[0m resnet18\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(resnet18\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features, NUM_CLASSES)\n\u001b[1;32m      8\u001b[0m squeezenet \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39msqueezenet1_0(weights\u001b[38;5;241m=\u001b[39mSqueezeNet1_0_Weights\u001b[38;5;241m.\u001b[39mIMAGENET1K_V1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ResNet18_Weights' is not defined"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "resnet18 = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, NUM_CLASSES)\n",
    "\n",
    "squeezenet = models.squeezenet1_0(weights=SqueezeNet1_0_Weights.IMAGENET1K_V1)\n",
    "squeezenet.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.7),\n",
    "    nn.Conv2d(512, NUM_CLASSES, kernel_size=1),\n",
    "    nn.Sigmoid(),\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    nn.Flatten()\n",
    ")\n",
    "\n",
    "mobilenetv3 = models.mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "mobilenetv3.classifier = nn.Sequential(\n",
    "    nn.Linear(mobilenetv3.classifier[0].in_features, NUM_CLASSES),\n",
    ")\n",
    "\n",
    "for model in [resnet18, squeezenet, mobilenetv3]:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    if isinstance(model, models.ResNet):\n",
    "        model.fc.weight.requires_grad = True\n",
    "        model.fc.bias.requires_grad = True\n",
    "    elif isinstance(model, models.SqueezeNet):\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif isinstance(model, models.MobileNetV3):\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Training]: 100%|██████████| 25/25 [00:03<00:00,  6.75it/s, train_loss=0.385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.3845, Val Loss: 0.7022\n",
      "Model weights saved for epoch 1 as 'pretrained/MobileNetV3_e1.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Training]: 100%|██████████| 25/25 [00:03<00:00,  7.13it/s, train_loss=0.301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 0.3010, Val Loss: 0.7747\n",
      "Model weights saved for epoch 2 as 'pretrained/MobileNetV3_e2.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Training]: 100%|██████████| 25/25 [00:03<00:00,  7.05it/s, train_loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 0.2882, Val Loss: 0.8377\n",
      "Model weights saved for epoch 3 as 'pretrained/MobileNetV3_e3.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Training]: 100%|██████████| 25/25 [00:03<00:00,  7.09it/s, train_loss=0.282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 0.2823, Val Loss: 0.7888\n",
      "Model weights saved for epoch 4 as 'pretrained/MobileNetV3_e4.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Training]: 100%|██████████| 25/25 [00:03<00:00,  7.06it/s, train_loss=0.274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 0.2745, Val Loss: 0.6749\n",
      "Model weights saved for epoch 5 as 'pretrained/MobileNetV3_e5.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Training]: 100%|██████████| 25/25 [00:03<00:00,  7.19it/s, train_loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Loss: 0.2770, Val Loss: 0.5481\n",
      "Model weights saved for epoch 6 as 'pretrained/MobileNetV3_e6.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Training]: 100%|██████████| 25/25 [00:03<00:00,  6.98it/s, train_loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Loss: 0.2707, Val Loss: 0.4383\n",
      "Model weights saved for epoch 7 as 'pretrained/MobileNetV3_e7.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Training]: 100%|██████████| 25/25 [00:03<00:00,  7.00it/s, train_loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Loss: 0.2710, Val Loss: 0.3688\n",
      "Model weights saved for epoch 8 as 'pretrained/MobileNetV3_e8.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Training]: 100%|██████████| 25/25 [00:03<00:00,  7.16it/s, train_loss=0.274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Loss: 0.2736, Val Loss: 0.3265\n",
      "Model weights saved for epoch 9 as 'pretrained/MobileNetV3_e9.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Training]: 100%|██████████| 25/25 [00:03<00:00,  7.19it/s, train_loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Loss: 0.2706, Val Loss: 0.3088\n",
      "Model weights saved for epoch 10 as 'pretrained/MobileNetV3_e10.pth'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3263, Test Accuracy: 17.40%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.32630416192114353, 17.4)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train_losses, val_losses = train_model(resnet18, train_loader, val_loader, learning_rate=0.001, num_epochs=10, pretrained=True)\n",
    "# test_model(resnet18, test_loader)\n",
    "\n",
    "# train_losses, val_losses = train_model(squeezenet, train_loader, val_loader, learning_rate=0.001, num_epochs=10, pretrained=True)\n",
    "# test_model(squeezenet, test_loader)\n",
    "\n",
    "train_losses, val_losses = train_model(mobilenetv3, train_loader, val_loader, learning_rate=0.001, num_epochs=10, pretrained=True)\n",
    "test_model(mobilenetv3, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baptiste/anaconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Processing images in ../data_students/labeled_data/Img_train_cropped/:  23%|██▎       | 233/1000 [00:30<01:39,  7.72it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 90\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# squeezenet = load_model('squeezenet', f\"{PRETRAINED_PATH}SqueezeNet_e10.pth\")\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# process_images('train', squeezenet)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# process_images('test', squeezenet)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# resnet18 = load_model('resnet18', f\"{PRETRAINED_PATH}ResNet_e10.pth\")\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# process_images('train', resnet18)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# process_images('test', resnet18)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m mobilenetv3 \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmobilenetv3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPRETRAINED_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mMobileNetV3_e10.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m \u001b[43mprocess_images\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmobilenetv3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m process_images(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, mobilenetv3)\n",
      "Cell \u001b[0;32mIn[24], line 74\u001b[0m, in \u001b[0;36mprocess_images\u001b[0;34m(mode, model, cropped)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_file \u001b[38;5;129;01min\u001b[39;00m tqdm(image_files, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing images in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     73\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, img_file)\n\u001b[0;32m---> 74\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     feature_list\u001b[38;5;241m.\u001b[39mappend(features)\n\u001b[1;32m     76\u001b[0m     filenames\u001b[38;5;241m.\u001b[39mappend(img_file)\n",
      "Cell \u001b[0;32mIn[24], line 56\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(image_path, model)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 56\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/models/mobilenetv3.py:220\u001b[0m, in \u001b[0;36mMobileNetV3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/models/mobilenetv3.py:210\u001b[0m, in \u001b[0;36mMobileNetV3._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 210\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    213\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/models/mobilenetv3.py:111\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 111\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[1;32m    113\u001b[0m         result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "PRETRAINED_PATH = \"./pretrained/saved/\"\n",
    "\n",
    "def load_model(model_name, checkpoint_path, num_classes=NUM_CLASSES):\n",
    "    if model_name == 'squeezenet':\n",
    "        model = models.squeezenet1_0(weights=None)\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "    elif model_name == 'resnet18':\n",
    "        model = models.resnet18(weights=None)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "    elif model_name == 'mobilenetv3':\n",
    "        model = models.mobilenet_v3_small(weights=None)\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Linear(model.classifier[0].in_features, 1),\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model name: {model_name}\")\n",
    "\n",
    "    state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'), weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    if model_name == 'squeezenet':\n",
    "        model.classifier = nn.Identity()\n",
    "    elif model_name == 'resnet18':\n",
    "        model.fc = nn.Identity()\n",
    "    elif model_name == 'mobilenetv3':\n",
    "        model.classifier = nn.Identity()\n",
    "    \n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    # image = image.resize((224, 224)) # input size for ResNet18\n",
    "    image = image.resize((28, 28))  # Resize for SqueezeNet/MobileNetV3\n",
    "    image_np = np.array(image).astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    image_np = (image_np - MEAN) / STD  # Normalize with MEAN and STD\n",
    "    image_np = np.transpose(image_np, (2, 0, 1))  # Convert HWC to CHW\n",
    "    image_tensor = torch.tensor(image_np, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "    return image_tensor\n",
    "\n",
    "def extract_features(image_path, model):\n",
    "    image_tensor = preprocess_image(image_path)\n",
    "    if image_tensor is None:\n",
    "        return None\n",
    "    with torch.no_grad():\n",
    "        features = model(image_tensor)\n",
    "    return features.numpy().flatten()\n",
    "\n",
    "def process_images(mode, model, cropped=True):\n",
    "    feature_list = []\n",
    "    filenames = []\n",
    "    \n",
    "    if cropped:\n",
    "        folder = f\"{FOLDER_INPUT_PATH}Img_{mode}_cropped/\"\n",
    "    else:\n",
    "        folder = f\"{FOLDER_INPUT_PATH}Img_{mode}/\"\n",
    "\n",
    "    output_csv = f\"{FOLDER_OUTPUT_PATH}img_feat_{get_variable_name(model, globals())}_{mode}.csv\"\n",
    "    \n",
    "    image_files = [img_file for img_file in sorted(os.listdir(folder)) if img_file.endswith('.png')]\n",
    "\n",
    "    for img_file in tqdm(image_files, desc=f\"Processing images in {folder}\"):\n",
    "        img_path = os.path.join(folder, img_file)\n",
    "        features = extract_features(img_path, model)\n",
    "        feature_list.append(features)\n",
    "        filenames.append(img_file)\n",
    "\n",
    "    feature_df = pd.DataFrame(feature_list)\n",
    "    feature_df['img_filename'] = filenames\n",
    "    feature_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Features saved to {output_csv}\")\n",
    "\n",
    "# squeezenet = load_model('squeezenet', f\"{PRETRAINED_PATH}SqueezeNet_e10.pth\")\n",
    "# process_images('train', squeezenet)\n",
    "# process_images('test', squeezenet)\n",
    "# resnet18 = load_model('resnet18', f\"{PRETRAINED_PATH}ResNet_e10.pth\")\n",
    "# process_images('train', resnet18)\n",
    "# process_images('test', resnet18)\n",
    "mobilenetv3 = load_model('mobilenetv3', f\"{PRETRAINED_PATH}MobileNetV3_e10.pth\")\n",
    "process_images('train', mobilenetv3)\n",
    "process_images('test', mobilenetv3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # Output: 32x32\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 16x16\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # Output: 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 8x8\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),  # Flatten from 32 channels x 8 x 8 to 2048\n",
    "            nn.Linear(32 * 8 * 8, 128),  # Fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),  # Output layer\n",
    "            nn.Sigmoid(),  # Sigmoid activation for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        return self.features(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNATrous(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNATrous, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=2, dilation=2),  # Output: 32x32\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 16x16\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=4, dilation=4),  # Output: 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 8x8\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),  # Flatten from 32 channels x 8 x 8 to 2048\n",
    "            nn.Linear(32 * 8 * 8, 128),  # Fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),  # Output layer\n",
    "            nn.Sigmoid(),  # Sigmoid activation for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        return self.features(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDoubleConv(nn.Module):\n",
    "    def __init__(self, input_size=(32, 32)):\n",
    "        super(CNNDoubleConv, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Downsample to input_size / 2\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Downsample to input_size / 4\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, *input_size)  # Batch size of 1, 1 channel\n",
    "            flattened_size = self.features(dummy_input).numel()  # Compute output size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        return self.features(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Training]:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Training]: 100%|██████████| 25/25 [00:01<00:00, 13.03it/s, train_loss=0.743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.7130, Val Loss: 0.6931\n",
      "Model weights saved for epoch 1 as 'pretrained/CNNDoubleConv_e1.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Training]: 100%|██████████| 25/25 [00:01<00:00, 14.56it/s, train_loss=0.693]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m _,_ \u001b[38;5;241m=\u001b[39m test_model(model, test_loader)\n",
      "Cell \u001b[0;32mIn[22], line 36\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, learning_rate, num_epochs, pretrained)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[1;32m     35\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(model(images))\n\u001b[0;32m---> 36\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     38\u001b[0m val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 32\u001b[0m, in \u001b[0;36mCNNDoubleConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 32\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(features)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# model = CNN()\n",
    "# model = CNNATrous()\n",
    "model = CNNDoubleConv()\n",
    "\n",
    "save_path = \"pretrained\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "train_losses, val_losses = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=10\n",
    ")\n",
    "_,_ = test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 1000/1000 [00:00<00:00, 1167.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to ../data_students/labeled_data/img_features/img_feat_CNNDoubleConv_train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 500/500 [00:00<00:00, 925.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to ../data_students/labeled_data/img_features/img_feat_CNNDoubleConv_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 500/500 [00:00<00:00, 1000.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to ../data_students/unlabeled_data/img_features/img_feat_CNNDoubleConv_predict.csv\n"
     ]
    }
   ],
   "source": [
    "def extract_features(model, dataset, output_csv):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    image_names = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(range(len(dataset)), desc=\"Extracting Features\"):\n",
    "            data = dataset[idx]\n",
    "            if isinstance(data, tuple):  # Si le dataset retourne (image, label)\n",
    "                image, _ = data\n",
    "            else:  # Sinon, il ne retourne que l'image\n",
    "                image = data\n",
    "            image_name = dataset.image_paths[idx].name if hasattr(dataset, 'image_paths') else f\"image_{idx}.png\"\n",
    "            \n",
    "            if not isinstance(image, torch.Tensor):  # Ensure the image is a tensor\n",
    "                image = transforms.ToTensor()(image)  # Convert image to tensor if it's not already\n",
    "\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            image = image.unsqueeze(0).to(device)\n",
    "\n",
    "            feature_vector = model.extract_features(image)\n",
    "            feature_vector = feature_vector.flatten().cpu().numpy()\n",
    "\n",
    "            features.append(feature_vector)\n",
    "            image_names.append(image_name)\n",
    "\n",
    "    features_array = np.array(features)\n",
    "    df = pd.DataFrame(features_array)\n",
    "    df.insert(0, \"img_filename\", image_names)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Features saved to {output_csv}\")\n",
    "\n",
    "epoch = 10                     # CHANGE EPOCH HERE\n",
    "has_data_augmentation = False   # CHANGE FOR DATA AUGMENTATION HERE\n",
    "data = \"\"\n",
    "if has_data_augmentation:\n",
    "    data = \"augmented_\"\n",
    "model = CNNDoubleConv()\n",
    "#model.load_state_dict(torch.load(f\"pretrained/{type(model).__name__}_e{epoch}.pth\", weights_only=True))\n",
    "output_csv_train = f\"{FOLDER_OUTPUT_PATH}img_feat_{type(model).__name__}_{data}train.csv\"\n",
    "extract_features(model, train_dataset, output_csv_train)\n",
    "\n",
    "output_csv_test = f\"{FOLDER_OUTPUT_PATH}img_feat_{type(model).__name__}_{data}test.csv\"\n",
    "extract_features(model, test_dataset, output_csv_test)\n",
    "\n",
    "output_csv_predict = f\"{FOLDER_OUTPUT_PATH_PREDICT}img_features/img_feat_{type(model).__name__}_{data}predict.csv\"\n",
    "extract_features(model, predict_dataset, output_csv_predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging tabular data and images features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging data for CNNDoubleConv features...\n",
      "Train data combined and saved to ../data_students/labeled_data/X_train_combined.csv.\n",
      "Test data combined and saved to ../data_students/labeled_data/X_test_combined.csv.\n",
      "Predict data combined and saved to ../data_students/unlabeled_data/X_predict_combined.csv.\n"
     ]
    }
   ],
   "source": [
    "def merge_data(mode, model, augmented=False):\n",
    "    if mode not in ['train', 'test', 'predict']:\n",
    "        raise ValueError(\"Mode must be either 'train' or 'test'.\")\n",
    "    \n",
    "    if mode == 'predict':\n",
    "        tabular_csv = f\"{FOLDER_INPUT_PATH_PREDICT}X.csv\"\n",
    "        feature_csv = f\"{FOLDER_OUTPUT_PATH_PREDICT}img_features/img_feat_{model}_{mode}.csv\"\n",
    "        if augmented:\n",
    "            feature_csv = f\"{FOLDER_OUTPUT_PATH_PREDICT}img_features/img_feat_{model}_augmented_{mode}.csv\"\n",
    "        combined_csv = f\"{FOLDER_INPUT_PATH_PREDICT}X_{mode}_combined.csv\"\n",
    "\n",
    "        tabular_data = pd.read_csv(tabular_csv)\n",
    "        image_features = pd.read_csv(feature_csv)\n",
    "\n",
    "        combined_data = pd.merge(tabular_data, image_features, on='img_filename', how='inner')\n",
    "        combined_data = combined_data.drop(columns=['img_filename'])\n",
    "\n",
    "        combined_data.to_csv(combined_csv, index=False)\n",
    "        print(f\"{mode.capitalize()} data combined and saved to {combined_csv}.\")\n",
    "    else:\n",
    "        tabular_csv = f\"{FOLDER_INPUT_PATH}X_{mode}.csv\"\n",
    "        feature_csv = f\"{FOLDER_OUTPUT_PATH}img_feat_{model}_{mode}.csv\"\n",
    "        if augmented:\n",
    "            feature_csv = f\"{FOLDER_OUTPUT_PATH}img_feat_{model}_augmented_{mode}.csv\"\n",
    "        combined_csv = f\"{FOLDER_INPUT_PATH}X_{mode}_combined.csv\"\n",
    "\n",
    "        tabular_data = pd.read_csv(tabular_csv)\n",
    "        image_features = pd.read_csv(feature_csv)\n",
    "\n",
    "        combined_data = pd.merge(tabular_data, image_features, on='img_filename', how='inner')\n",
    "        combined_data = combined_data.drop(columns=['img_filename'])\n",
    "\n",
    "        combined_data.to_csv(combined_csv, index=False)\n",
    "        print(f\"{mode.capitalize()} data combined and saved to {combined_csv}.\")\n",
    "\n",
    "models = {\n",
    "    1: 'flat',\n",
    "    2: 'hog',\n",
    "    3: 'orb',\n",
    "    4: 'sift',\n",
    "    5: 'ced',\n",
    "    6: 'harris',\n",
    "    7: 'CNN',\n",
    "    8: 'CNNAtrous',\n",
    "    9: 'CNNDoubleConv', #best\n",
    "    10: 'resnet18',\n",
    "    11: 'squeezenet',\n",
    "    12: 'mobilenetv3'\n",
    "}\n",
    "model = models[9]  # CHANGE MODEL HERE\n",
    "print(f\"Merging data for {model} features...\")\n",
    "merge_data('train', model, augmented=True)\n",
    "merge_data('test', model, augmented=True)",
    "merge_data('predict', model, augmented=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
